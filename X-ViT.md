# X-ViT

最近Transformer在视频识别领域的尝试在识别精度方面展现出了非常不错的结果，但在许多情况下，由于时间维度的额外建模，会导致显著的计算开销提升。

作者提出了一个视频Transformer模型，该模型的复杂度与视频序列中的帧数呈线性的关系，因此与基于图像的Transformer模型相比，不会产生额外的计算开销。为了实现这一点，本文的视频Transformer对 full space-time attention进行了两个方面的近似：
a)它将时间注意力限制在一个局部时间窗口 ，并利用Transformer的深度来获得视频序列的全时间覆盖

b)它使用有效的时空混合来联合建模空间和时间信息 ，与仅空间注意模型相比没有产生任何额外的计算成本。作者通过实验证明了，本文的模型在视频识别数据集上产生了非常高的精度


2. 

视觉Transformer在时空领域的一个直接的、自然的扩展是在所有的S个空间位置和T个时间位置上共同进行Self-Attention。但是全时空注意具有的复杂性，这使得模型计算起来很沉重，甚至比3D CNN的复杂度还要高

这个问题的一个基本解决方案是只考虑空间注意，然后在时间维度上平均，它具有的复杂度（上图（b））。还有一些方法（上图（c））在视频识别精度方面已经显示出了很不错的结果，但在大多数情况下，由于时间信息的额外建模，与baseline（仅空间建模）方法相比，它们还是会导致显著的计算开销。

3. 

基于ViT的处理方法，每一帧被分成K×K个不重叠的patch，然后使用线性embedding层将patch映射到视觉token。

由于Self-Attention是排列不变的，为了保留每个patch在空间和时间内的位置信息，作者还用了两个可学习的位置embedding，一个用于空间：，一个用于时间：。然后将这些token添加到初始的视觉token中，用L个Transformer layers进行信息的建模

第l层、空间位置s和时间位置t处的视觉token可以表示:

一个全时空自注意(SA) head的计算可以表示为

4. 

本文的模型旨在保持复杂度的同时，更好地近似完全的时空自注意，即不增加仅空间自注意的复杂度。为了实现这一点，作者首先进行了一个近似操作来执行全时空注意，但将时间注意的范围限制在一个局部时间窗口，：

作者在第一个近似之上做了第二个近似。上式中位置和的attention表示如下：

它需要计算个位置的attention，因此在时间维度上依旧多了计算局部时间attention的复杂度。在计算，的时间区间上，作者采用了一个单一的attention，来减少计算复杂度。

具体实现上，作者采用了“shift trick”，这使得不需要引入任何额外的参数和计算量，就能融合时间和空间维度。


5.

上图展示了新的key 形成的过程，用相似的方法，还可以构建新的value最后，新的全时空注意的近似方法计算如下：

通过这两种近似方法，就可以获得的复杂度，因此它比以前提出的视频Transformer更有效。除了更小的计算量，本文的方法还能获得更高的精度。

6. 

模型使用class token 来生成预测。作者尝试了以下几种方式来聚合时间的信息：

作者尝试了以下几种方式来聚合时间的信息：

1）在时间维度上简单进行时间维度的平均：

输出就是每帧特征的集合，因此，完全忽略了它们之间的时间顺序。为了解决这个问题，作者使用了一种轻量级的时间注意(TA)机制

它将attend T个class token。在实现上，使用时间Transformer来attend到序列，，，将其作为分类器的输入。

7. 

作为TA的替代方案，作者还提出了一种简单的轻量级机制，用于网络中间层不同帧之间的信息交换。给定第t帧的token集合，作者计算了一组新的长度为的token ，φ，它对token信息进行了总结（记为“Summary” tokens）。

然后将这些“Summary”  tokens接到query，key，value上，以便query能够直接attend到带有“Summary” tokens的key。其中φ为简单的空间上的平均，如下所示


时空混合注意和轻量级全局时间注意（或Summary token）的视频Transformer称为**X-ViT** 。



focus on:maskfeat,

long-dependencasasd

Uiformer former结构

parameter effiecient